MASK_TOKEN = '<MASK>'
CLS_TOKEN = '<CLS>'
SEP_TOKEN = '<SEP>'
PAD_TOKEN = '<pad>'

MAX_MASK_NUMBER = 20
SEQUENCE_LENGTH = 255

BATCH_SIZE = 16
TARGET_BATCH_SIZE = 256
WARMUP_STEPS = 2000
TOTAL_STEPS = 1000000
LR = 5e-5
MODEL_SAVE_PATH = 'model/'
SAVE_STEP = 10000
TIME_SAVE_STEP = 1000
NUM_WORKERS = 3

VOCAB_SIZE = 50000
MAX_SUBTOKEN_LENGTH = 15
VOCAB_PATH = 'pretraining_data/vocab.model'
DATA_PATH = 'pretraining_data/sentences.json'