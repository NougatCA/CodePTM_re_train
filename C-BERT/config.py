MASK_TOKEN = '[MASK]'
PAD_TOKEN = '[pad]'

MAX_MASK_NUMBER = 20
SEQUENCE_LENGTH = 512

BATCH_SIZE = 32
TARGET_BATCH_SIZE = 256
WARMUP_STEPS = 2000
TOTAL_STEPS = 1000000
LR = 5e-5
MODEL_SAVE_PATH = 'model/'
SAVE_STEP = 5000
TIME_SAVE_STEP = 1000
NUM_WORKERS = 2

VOCAB_SIZE = 50000
MAX_SUBTOKEN_LENGTH = 15
VOCAB_PATH = 'pretraining_data/vocab.model'
DATA_PATH = 'pretraining_data/sentences.json'
